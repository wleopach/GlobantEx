<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: NRI: INT: Dense 3D Reconstruction of Dynamic Actors in Natural Environments using  Multiple Flying Cameras</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>638460.00</AwardTotalIntnAmount>
<AwardAmount>638460</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>While large-scale multi-camera domes have been developed for data collection in controlled laboratory settings it is not possible to achieve a similar level of measurement quality outdoors where there is much potential benefit to such data collection. For example, use of such measurements include the body dynamics of a running cheetah, or people, or analyzing herding behaviors of animals or birds. This leads to scientists relying on extremely inefficient and dangerous data collection methods. For example, biologists studying the behaviors of wild animals try to predict where the animals will be and place some cameras which only give some limited data at specific locations. This project addresses such challenges by exploring the research of methods and development of a large-scale data collection tool for high-resolution and multi-viewpoint visual recording and motion analysis of natural group behaviors (e.g., herds of animals or groups of people) in-the-wild over very large environments (e.g., desert plains or mountain sides) using a team of flying robots. &lt;br/&gt;&lt;br/&gt;This project develops computational models that integrate the fundamentals of computer vision and multi-agent control to measure the group of actors in 3D. Through the development of this system, this project will make major advances in technology at the intersection of perception and control that include: (1) a new study of methods for precise, rapid, and robust target motion forecasting and relative state estimation that estimates the 3D motion of the robots and actors quickly with strong uncertainty estimates; (2) a new decomposition of the perception-aware multi-objective multi-UAV safe motion planning problem, that allows long-term planning based on consistent actor forecasting uncertainty models and coverage objectives; (3) a new guaranteed safe but adaptive paradigm for reactive flight control that is able to generate safety maneuvers even under large disturbances and vehicle dynamics changes, and that can leverage prior flight experience for real-time adaptation; (4) new theory of 3D reconstruction for dynamic scenes captured by UAVs that will enable high-resolution mesh and skeletal reconstruction of the groups of actors. The research outcome will be disseminated through multiple educational activities.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/10/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/13/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2022894</AwardID>
<Investigator>
<FirstName>Ibrahim</FirstName>
<LastName>Isler</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ibrahim V Isler</PI_FULL_NAME>
<EmailAddress>isler@cs.umn.edu</EmailAddress>
<PI_PHON>6126251067</PI_PHON>
<NSF_ID>000233463</NSF_ID>
<StartDate>09/10/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Hyun Soo</FirstName>
<LastName>Park</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Hyun Soo Park</PI_FULL_NAME>
<EmailAddress>hspark@umn.edu</EmailAddress>
<PI_PHON>6123011745</PI_PHON>
<NSF_ID>000741915</NSF_ID>
<StartDate>09/10/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<CountyName>HENNEPIN</CountyName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<CountyName>HENNEPIN</CountyName>
<StateCode>MN</StateCode>
<ZipCode>554550153</ZipCode>
<StreetAddress><![CDATA[100 Union St SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~638460</FUND_OBLG>
</Award>
</rootTag>
