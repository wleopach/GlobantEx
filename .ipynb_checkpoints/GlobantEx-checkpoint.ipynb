{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06618689",
   "metadata": {},
   "source": [
    "## Importamos las librerías que se van a usar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "black-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys, csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from scipy.special import psi\n",
    "from wordcloud import WordCloud\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-morrison",
   "metadata": {},
   "source": [
    "\n",
    "## Cargamos los documentos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "organized-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('AbstractsClassS/*.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-awareness",
   "metadata": {},
   "source": [
    "## Definimos algunas funciones para procesar los documentos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demanding-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La siguiente funcion devuelve el abstract de un archivo que se le introduzca.\n",
    "def getabstract(filename):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~123456789'''\n",
    "    try:\n",
    "        tree = ET.parse(filename, parser= ET.XMLParser(encoding = 'iso-8859-5'))\n",
    "        root = tree.getroot()\n",
    "    except:\n",
    "        return \" \"\n",
    "    doc = \" \"\n",
    "    for i in root[0]:\n",
    "        if i.tag == 'AbstractNarration' and i.text is not None:\n",
    "            doc = i.text\n",
    "            doc = doc.replace('<br/>', ' ')\n",
    "            doc = doc.replace('т\\x80\\x99',' ')\n",
    "            doc = doc.replace('т\\x80\\x94', ' ')\n",
    "            doc = doc.replace('т\\x80\\x9c', ' ')\n",
    "            doc = doc.replace('т\\x80\\x9d', ' ')\n",
    "            no_punt = \"\"\n",
    "            for char in doc:\n",
    "                if char not in punctuations:\n",
    "                    no_punt = no_punt + char\n",
    "            doc = no_punt\n",
    "            doc = doc.lower()\n",
    "            doc = ' '.join([word for word in doc.split() if word not in stopwords.words('english')])\n",
    "    # print doc\n",
    "    return doc\n",
    "#la siguiente funci'on devuelve el título de un artícilo dado\n",
    "\n",
    "\n",
    "def gettitle(filename):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~123456789'''\n",
    "    try:\n",
    "        tree = ET.parse(filename, parser= ET.XMLParser(encoding = 'iso-8859-5'))\n",
    "        root = tree.getroot()\n",
    "    except:\n",
    "        return \" \"\n",
    "    doc = \" \"\n",
    "    for i in root[0]:\n",
    "        if i.tag == 'AwardTitle' and i.text is not None:\n",
    "            doc = i.text\n",
    "            doc = doc.lower()\n",
    "            doc = ' '.join([word for word in doc.split() if word not in stopwords.words('english')])\n",
    "            no_punt = \"\"\n",
    "            for char in doc:\n",
    "                if char not in punctuations:\n",
    "                    no_punt = no_punt + char\n",
    "            doc = no_punt\n",
    "    # print doc\n",
    "    return doc\n",
    "\n",
    "#La siguiente funcion  devuelve una lista con todos los abstracts no vacios\n",
    "def getalldocs():\n",
    "    files = filenames\n",
    "    docs = []\n",
    "    for file in files:\n",
    "        doc = getabstract(file)\n",
    "        # print doc\n",
    "        if doc != \" \":\n",
    "            docs.append(doc)\n",
    "    # print docs\n",
    "    return docs\n",
    "\n",
    "\n",
    "#La siguiente funci'on devuelve todos los titulos del premio de un articulo dado\n",
    "\n",
    "def getalltitles():\n",
    "    files = filenames\n",
    "    titles = []\n",
    "    for file in files:\n",
    "        tit = gettitle(file)\n",
    "        doc = getabstract(file)\n",
    "        # print doc\n",
    "        if doc != \" \":\n",
    "            titles.append(tit)\n",
    "    # print docs\n",
    "    return titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-justice",
   "metadata": {},
   "source": [
    "## Ahora procesamos los datos y hacemos un word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pegamos las dos listas\n",
    "listTitle_Abstract = {'titles': getalltitles(), 'abstract': getalldocs()}\n",
    "#Convertimos la lista en un data frame\n",
    "abstTitleDF = pd.DataFrame(listTitle_Abstract, columns=['titles', 'abstract'])\n",
    "# Pegamos todos las palabras procesadas.\n",
    "long_string = ','.join(list(abstTitleDF['titles'].values))\n",
    "long_string += ','.join(list(abstTitleDF['abstract'].values))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=10000, contour_width=3, contour_color='steelblue')\n",
    "# Generamos un word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualizaci'on del word cloud\n",
    "wordcloud.to_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-reach",
   "metadata": {},
   "source": [
    "## Ahora contamos las palabras más repetidas en los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts += t.toarray()[0]\n",
    "\n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x: x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words))\n",
    "\n",
    "    plt.figure(2, figsize=(15, 15 / 1.6180))\n",
    "    plt.subplot(title='Las diez palabras más usadas')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90)\n",
    "    plt.xlabel('Palabras')\n",
    "    plt.ylabel('Cantidad')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Inicializamos el count vectorizer cpn las English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# Fit and transform de los documentos procesados\n",
    "count_data = count_vectorizer.fit_transform(long_string.split())\n",
    "# Visualisamos las 10 palabras mas usadas\n",
    "plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-playing",
   "metadata": {},
   "source": [
    "## Ahora aplicamos Latent Diriclet Allocation a los documentos, para clasificar los documentos por temas, sería bueno poder comparar varios escenarios y escoger el número de temas de la mejor manera posible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La siguiente funci'on  devuelve los temas datos por lda y sus palabras\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "# Definimos el numero de temas y palabras por tema\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "# Creamos y ajustamos el LDA\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Imprimimos los temas y las palabras\n",
    "print(\"Temas encontrados  via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-eagle",
   "metadata": {},
   "source": [
    "## Finalmente guardamaos en un diccionario los documentos con su tema asignado por el algoritmo, después graficamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = lda.transform(count_data)\n",
    "r = dict()\n",
    "for i in range(number_topics):\n",
    "    r[i]=[]\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    r[topic_most_pr].append(n)\n",
    "\n",
    "s=[]\n",
    "for i in range(number_topics):\n",
    "    s.append(len(r[i]))\n",
    "# Data to plot\n",
    "labels = list(range(number_topics))\n",
    "sizes = s\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, labels=labels,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
