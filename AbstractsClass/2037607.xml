<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>I-Corps: Artificial Intelligence-Driven Disaster Risk Prediction and Assessment</AwardTitle>
<AwardEffectiveDate>07/15/2020</AwardEffectiveDate>
<AwardExpirationDate>12/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>50000.00</AwardTotalIntnAmount>
<AwardAmount>50000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ruth Shuman</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this I-Corps project is the development of a rapid and automated scene understanding technology to intelligently evaluate existing conditions of vulnerability and assess potential disaster risks more proactively and effectively.  This technology may help make informed decisions regarding steps to reduce the impacts of disasters. Developing and implementing the technology to reduce disaster risk requires identifying and assessing the sources of potential risk effectively, but a lack of a rapid and automated tools makes practitioners rely on manual inspection. The proposed innovation is to make the current disaster risk assessment process more intelligent and efficient by reducing time-consuming and labor-intensive manual inspection. If the project is completed successfully, the proposed innovation will enhance scientific and technological understanding of rapid and automated visual sensing and analytics for disaster risk prediction and assessment.&lt;br/&gt;&lt;br/&gt;This I-Corps project is based on the development of the integrated analysis of low-level image features together with high-level semantic models, which enables robust scene understanding and risk prediction in complex environments. By leveraging large-scale data from multimodal visual sensors (e.g., drones, security cameras, etc.), the technology automatically encodes the context of potential disaster risk into machine vision algorithms to identify the elements at risk on video recordings and assess the degree of vulnerability and the elements at risk in complex environments. The system generates site-specific managerial information in an automated manner, which may enhance risk-informed decision-making for disaster mitigation and preparedness, thereby reducing both the level of risk and the impacts of disasters effectively. The proposed innovation may also provide the foundation for further vision-based reasoning for disaster management.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/22/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/22/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2037607</AwardID>
<Investigator>
<FirstName>Youngjib</FirstName>
<LastName>Ham</LastName>
<EmailAddress>yham@tamu.edu</EmailAddress>
<StartDate>07/22/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M University</Name>
<CityName>College Station</CityName>
<ZipCode>778454375</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy South</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
</Institution>
<ProgramElement>
<Code>8023</Code>
<Text>I-Corps</Text>
</ProgramElement>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
