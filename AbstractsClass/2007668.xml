<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF: Small: Computationally Efficient Second-Order Optimization Algorithms for Large-Scale Learning</AwardTitle>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>500000.00</AwardTotalIntnAmount>
<AwardAmount>500000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Armand Makowski</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The rapid success of machine learning and artificial intelligence has positively affected several domains such as robotics, wireless communications, and sensor networks, to name a few. This success is mostly due to advances in storage, computational power, data representation, and algorithms, which allows the power of increasingly rich datasets to be harnessed. In particular, advances in computationally efficient optimization algorithms have had a crucial role in this success, as most tasks in modern machine learning and artificial intelligence problems can be formulated as optimization programs. Despite significant progress, most existing optimization algorithms could be slow when applied to the ill-conditioned problems that often arise in large-scale machine learning. This project lays out an agenda to develop a class of memory efficient, computationally affordable, and distributed friendly second-order methods for solving modern machine learning problems. On the education front, this project will provide a stimulating and innovative research environment for both graduate and undergraduate students; it will also incorporate the development of curricular material for courses at the University of Texas at Austin. &lt;br/&gt;&lt;br/&gt;Current optimization algorithms for large-scale machine learning are inefficient at times since these methods operate using only first-order information (gradient) of the objective function. This project aims to develop a class of fast and efficient second-order methods that exploit the curvature information of the objective function to accelerate convergence in ill-conditioned settings. The research encompasses three different thrusts: (I) Developing memory efficient incremental quasi-Newton methods with provably fast convergence guarantees; (II) Improving the computational complexity of second-order adaptive sample size algorithms by leveraging quasi-Newton approximation techniques; and (III) Designing distributed second-order methods that outperform first-order algorithms both in terms of overall complexity (in convex settings) and in terms of quality of solution (in non-convex settings).&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/13/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/13/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2007668</AwardID>
<Investigator>
<FirstName>Aryan</FirstName>
<LastName>Mokhtari</LastName>
<EmailAddress>mokhtari@austin.utexas.edu</EmailAddress>
<StartDate>07/13/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Texas at Austin</Name>
<CityName>Austin</CityName>
<ZipCode>787595316</ZipCode>
<PhoneNumber>5124716424</PhoneNumber>
<StreetAddress>3925 W Braker Lane, Ste 3.340</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
</Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
</Award>
</rootTag>
