<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: CHS: Collaboratively Perceiving, Comprehending, and Projecting into the Future: Supporting Team Situational Awareness with Adaptive Collaborative Tactons</AwardTitle>
<AwardEffectiveDate>08/12/2019</AwardEffectiveDate>
<AwardExpirationDate>05/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>15734.00</AwardTotalIntnAmount>
<AwardAmount>15734</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Data overload, especially in the visual channel, and associated breakdowns in monitoring already represent a major challenge in data-rich environments.  One promising means of overcoming data overload is through the introduction of multimodal displays which distribute information across various sensory channels (including vision, audition, and touch).  In recent years, touch has received more attention as a means to offload the overburdened visual and auditory channels, but much remains to be discovered in this modality.  Tactons, or tactile icons/displays, are structured, abstract messages that can be used to communicate information in the absence of vision.  However, the effectiveness of tactons may be compromised if their design does not take into account that complex systems depend on the coordinated activities of a team.  The PI's goal in this project is to establish a research program that will explore adaptive collaborative tactons as a means to support situational awareness, that is the ability of a team to perceive and comprehend information from the environment and predict future events in real time.  Project outcomes will contribute to a deeper understanding of perception and attention between and across sensory channels for individuals and teams, and to how multimodal interfaces can support teamwork in data-rich domains.&lt;br/&gt;&lt;br/&gt;The work will integrate three disparate topics within human factors: multimodal interfaces, situational awareness, and adaptive systems.   The PI will create methods to design tactons that take into account both context and the types of information needed by a team, by leveraging the multimodal aspects to develop quantitative and qualitative models and algorithms using physiological measures (in particular, eye tracking data).  These in turn will inform the functionality of adaptive tactons that support collaboration by adjusting the presentation of information in response to various sensed parameters and conditions.</AbstractNarration>
<MinAmdLetterDate>10/31/2019</MinAmdLetterDate>
<MaxAmdLetterDate>10/31/2019</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2002348</AwardID>
<Investigator>
<FirstName>Sara</FirstName>
<LastName>Riggs</LastName>
<EmailAddress>sriggs@virginia.edu</EmailAddress>
<StartDate>10/31/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Virginia Main Campus</Name>
<CityName>CHARLOTTESVILLE</CityName>
<ZipCode>229044195</ZipCode>
<PhoneNumber>4349244270</PhoneNumber>
<StreetAddress>P.O.  BOX 400195</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Virginia</StateName>
<StateCode>VA</StateCode>
</Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
</Award>
</rootTag>
