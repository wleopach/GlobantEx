<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Accelerating Machine Learning via Randomized Automatic Differentiation</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Machine learning is having a tremendous impact on our society and economy, but it depends critically on the ability to efficiently fit a model to data.  The technique of automatic differentiation takes software code written to build such a model and automatically performs the calculus derivations necessary to fit it to data. Automatic differentiation tools have been at the heart of the resurgence of neural networks for tackling problems ranging from drug discovery to self-driving cars.  This project revisits core assumptions in the way that automatic differentiation works, and identifies new ways that it can take advantage of randomness to find better machine learning models, faster.  This research will lead to new tools that expand the frontier of what machine learning systems are possible.&lt;br/&gt;&lt;br/&gt;The project will develop new techniques for automatic differentiation when it will be used as part of a stochastic optimization procedure, as is commonly done in training deep neural networks.  Rather than exact Jacobian accumulation on the linearized computational graph, this project proposes techniques for selecting random subgraphs such that the Jacobian is preserved in expectation but much less memory and computation is required.  Beyond randomization of the central Jacobian accumulation problem, the project will also explore how randomization can enable new approaches to implicit differentiation as used in PDE-constrained optimization and related problems. Additionally, the project will develop techniques for finding good, approximate near-optimal dynamic programming schedules for the linearized computational graph.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/03/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/15/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2007278</AwardID>
<Investigator>
<FirstName>Ryan</FirstName>
<LastName>Adams</LastName>
<EmailAddress>rpa@princeton.edu</EmailAddress>
<StartDate>09/03/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
