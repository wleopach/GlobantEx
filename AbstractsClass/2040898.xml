<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>FAI: End-To-End Fairness for Algorithm-in-the-Loop Decision Making in the Public Sector</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2021</AwardEffectiveDate>
<AwardExpirationDate>01/31/2024</AwardExpirationDate>
<AwardTotalIntnAmount>625000.00</AwardTotalIntnAmount>
<AwardAmount>625000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balakrishnan Prabhakaran</SignBlockName>
<PO_EMAI>bprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032924847</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to develop methods and tools that assist public sector organizations with fair and equitable policy interventions. In areas such as housing and criminal justice, critical decisions that impact lives, families, and communities are made by a variety of actors, including city officials, police, and court judges. In these high-stakes contexts, human decision makersâ€™ implicit biases can lead to disparities in outcomes across racial, gender, and socioeconomic lines. While artificial intelligence (AI) offers great promise for identifying and potentially correcting these sorts of biases, a rapidly growing literature has shown that automated decision tools can also worsen existing disparities or create new biases. To help bridge this gap between the promise and practice of AI, the interdisciplinary team of investigators will develop an integrated framework and new methodological approaches to support fair and equitable decision-making. This framework is motivated by three main ideas: (1) identifying and mitigating the impacts of biases on downstream decisions and their impacts, instead of simply measuring biases in data and in predictive models; (2) enabling the combination of an algorithmic decision support tool and a human decision-maker to make fairer and more equitable decisions than either human or algorithm alone; and (3) developing operational definitions of fairness and quantitative assessments of bias, guided by stakeholder discussions, that are directly relevant and applicable to the housing and criminal justice domains. The ultimate impact of this work is to advance social justice for those who live in cities, and who rely on city services or are involved with the justice system, by assessing and mitigating biases in decision-making processes and reducing disparities.&lt;br/&gt;&lt;br/&gt;The project team will address both the risks and the benefits of algorithmic decision-making through transformative technical contributions. First, they will develop a new, pipelined conceptualization of fairness consisting of seven distinct stages: data, models, predictions, recommendations, decisions, impacts, and outcomes. This end-to-end fairness pipeline will account for multiple sources of bias, model how biases propagate through the pipeline to result in inequitable outcomes and assess sensitivity to unmeasured biases. Second, they will build a general methodological framework for identifying and correcting biases at each stage of this pipeline, assessing intersectional and contextual biases across multiple data dimensions, and incorporating new ideas for model assessment and analysis of heterogeneous treatment effects. This generalized bias scan will provide essential information throughout the end-to-end fairness pipeline, informing not only what human and algorithmic biases exist, but what interventions are likely to mitigate these biases. Third, the project addresses algorithm-in-the-loop decision processes, in which an algorithmic decision support tool provides recommendations to a human decision-maker. The investigators will develop approaches for modeling systematic biases in human decisions, identifying possible explanatory factors for those biases, and optimizing individualized algorithmic "nudges" to guide human decisions toward fairness. Finally, the project team will create new metrics for measuring the presence and extent of bias. The outputs of the project will be designed for integration into the operational decision-making of city agencies responsible for making fair and equitable decisions in the criminal justice and housing domains. The investigators will assess the fairness of existing practices and create open-source tools for assessing and correcting biases, for users in each domain. They will develop tools which can be used to (a) reduce incarceration by equitably providing supportive interventions to justice-involved populations; (b) prioritize housing inspections and repairs; (c) assess and improve the fairness of civil and criminal court proceedings; and (d) analyze the disparate health impacts of adverse environmental exposures, including poor-quality housing and aggressive, unfair policing practices. Operational deployments of the developed tools will be regularly and comprehensively evaluated to assess impacts and to avoid unintended consequences, both maximizing the benefits and minimizing potential harms from both algorithmic and human decisions.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>01/25/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2040898</AwardID>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Neill</LastName>
<PI_MID_INIT>B</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel B Neill</PI_FULL_NAME>
<EmailAddress>daniel.neill@nyu.edu</EmailAddress>
<PI_PHON>6469970588</PI_PHON>
<NSF_ID>000484498</NSF_ID>
<StartDate>01/25/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Constantine</FirstName>
<LastName>Kontokosta</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Constantine E Kontokosta</PI_FULL_NAME>
<EmailAddress>ckontokosta@nyu.edu</EmailAddress>
<PI_PHON>2129982121</PI_PHON>
<NSF_ID>000649995</NSF_ID>
<StartDate>01/25/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ravi</FirstName>
<LastName>Shroff</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ravi Shroff</PI_FULL_NAME>
<EmailAddress>ravi.shroff@nyu.edu</EmailAddress>
<PI_PHON>2129982121</PI_PHON>
<NSF_ID>000684500</NSF_ID>
<StartDate>01/25/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Edward</FirstName>
<LastName>McFowland</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>III</PI_SUFX_NAME>
<PI_FULL_NAME>Edward McFowland</PI_FULL_NAME>
<EmailAddress>mcfowland@umn.edu</EmailAddress>
<PI_PHON>6126246872</PI_PHON>
<NSF_ID>000804391</NSF_ID>
<StartDate>01/25/2021</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>New York University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100121019</ZipCode>
<PhoneNumber>2129982121</PhoneNumber>
<StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041968306</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>NEW YORK UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041968306</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[New York University]]></Name>
<CityName>Brooklyn</CityName>
<StateCode>NY</StateCode>
<ZipCode>112013828</ZipCode>
<StreetAddress><![CDATA[370 Jay Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>08</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY08</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>114Y</Code>
<Text>Fairness in Artificial Intelli</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~625000</FUND_OBLG>
</Award>
</rootTag>
