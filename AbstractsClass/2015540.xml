<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Accurate and Interpretable Machine Learning for Prediction and Precision Medicine</AwardTitle>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>08/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>219995.00</AwardTotalIntnAmount>
<AwardAmount>219995</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Huixia Wang</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Many machine learning technologies are built using black-box approaches, which can make it difficult to scrutinize the technology's decision-making process. This lack of interpretability represents a fundamental barrier to the adoption of machine learning technologies in some areas, such as health care, where transparency is key. Researchers have long relied on decision trees as a means of interpretable machine learning. In this approach, one develops a series of yes/no questions that eventually lead to a particular action being taken. While appealing in their simplicity, researchers have generally accepted that this approach will perform more poorly than more complex (but less interpretable) approaches. This project will establish that this need not be the case. A new approach to creating decision trees will be developed, which has a strong theoretical basis and performance competitive with less interpretable algorithms that are considered state-of-the-art. Software to implement the new approach will be developed and made freely available. The developed methods will be applied to ongoing studies of preventive vaccines and are poised to have broad impacts by providing personalized recommendations for vaccination. The project will also support graduate students and develop pedagogical material pertaining to ethical issues arising in machine learning in public health and clinical care. &lt;br/&gt;&lt;br/&gt;The typical process for building a decision tree involves recursively partitioning the feature space using a greedy search. While this approach conveniently yields a decision tree, it is generally accepted to have worse performance when compared to other tree-based strategies, such as random forests or boosted trees. The project seeks to develop an alternative approach, where the feature space is partitioned using a method based on penalization called the highly adaptive lasso. The resultant prediction function enjoys desirable theoretical properties, but is not immediately representable as a decision tree, as it relies on non-recursive partitioning of the feature space. The first aim of the project will develop strategies for optimally representing a given partitioning via a recursive partitioning, thereby allowing a decision tree representation. A novel application of deep learning will be used to learn an optimal strategy for this representation, leveraging this archetypal uninterpretable algorithm to generate interpretable machine learning. In the second aim of the project, the approach will be extended to the context of personalized medicine and optimal decision trees for assigning treatments will be developed. The developments will have broad impacts on the theory of causal inference and robust machine learning. In the final aim, the developed methods will be applied to several contemporary trials of preventive vaccines to develop personalized vaccination recommendations. In particular, the methods will be applied to help determine optimal dosing strategies for a preventive malaria vaccine in children, which could have broad impacts on informing future vaccination strategies in Sub-Saharan Africa.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/23/2020</MinAmdLetterDate>
<MaxAmdLetterDate>06/23/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2015540</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Benkeser</LastName>
<EmailAddress>benkeser@emory.edu</EmailAddress>
<StartDate>06/23/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Emory University</Name>
<CityName>Atlanta</CityName>
<ZipCode>303224250</ZipCode>
<PhoneNumber>4047272503</PhoneNumber>
<StreetAddress>1599 Clifton Rd NE, 4th Floor</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
</Award>
</rootTag>
