<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: RI: Small: Modeling and Learning Ethical Principles for Embedding into Group Decision Support Systems</AwardTitle>
<AwardEffectiveDate>01/01/2021</AwardEffectiveDate>
<AwardExpirationDate>12/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>166411.00</AwardTotalIntnAmount>
<AwardAmount>166411</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Roger Mailler</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Many settings in everyday life require making decisions by combining the subjective preferences of individuals in a group, such as where to go to eat, where to go on vacation, whom to hire, which ideas to fund, or what route to take. In many domains, these subjective preferences are combined with moral values, ethical principles, or business constraints that are applicable to the decision scenario and are often prioritized over the preferences.  The potential conflict of moral values with subjective preferences are keenly felt both when AI systems recommend products to us and when we use AI enabled systems to make group decisions.  This research seeks to make AI more accountable by providing mechanisms to bound the decisions that AI systems can make, ensuring that the outcomes of the group decision making process aligns with human values.  To achieve the goal of building ethically-bounded, AI-enabled group decision making systems, this project takes inspiration from humans, who often constrain their decisions and actions according to a number of exogenous priorities coming from moral, ethical, or business values.  This research project will address the current lack of principled, formal approaches for embedding ethics into AI agents and AI enabled group decision support systems by advancing the state of the art in the safety and robustness of AI agents which, given how broadly AI touches our daily lives, will have broad impact and benefit to society.&lt;br/&gt;&lt;br/&gt;Specifically, the long-term goal of this project is to establish mathematical and machine learning foundations for embedding ethical guidelines into AI for group decision-making systems.  Within the machine ethics field there are two main approaches: the bottom-up approach focused on data-driven machine learning techniques and the top-down approach following symbolic and logic-based formalisms. This project brings these two methodologies closer together through three specific aims. (1) Modeling and Evaluating Ethical Principles: this project will extend principles in social choice theory and fair division using preference models from the literature on knowledge representation and preference reasoning. (2) Learning Ethical Principles From Data: this project will develop novel machine-learning frameworks to learn individual ethical principles and then aggregate them for use in group decision making systems. And finally, (3) Embedding Ethical Principles into Group Decision Support Systems: this project will develop novel frameworks for designing AI-based mechanisms for ethical group decision-making.  This research will establish novel methods for the formal and experimental unification of aspects of the top-down or rule-based approach with the bottoms-up or data-based approach for embedding ethics into group decision making systems.  The project will also formalize a framework for ethical and constrained reasoning across teams of computational agents.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/09/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2008011</AwardID>
<Investigator>
<FirstName>Kristen</FirstName>
<LastName>Venable</LastName>
<EmailAddress>bvenable@uwf.edu</EmailAddress>
<StartDate>09/09/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Florida Institute for Human and Machine Cognition, Inc.</Name>
<CityName>Pensacola</CityName>
<ZipCode>325026008</ZipCode>
<PhoneNumber>8502024473</PhoneNumber>
<StreetAddress>40 S. Alcaniz St.</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
