<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Self-supervised Representation Learning for Deformable Object Manipulation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/15/2021</AwardEffectiveDate>
<AwardExpirationDate>02/28/2026</AwardExpirationDate>
<AwardTotalIntnAmount>597151.00</AwardTotalIntnAmount>
<AwardAmount>397151</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
<PO_EMAI>damiller@nsf.gov</PO_EMAI>
<PO_PHON>7032924914</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Deformable objects; that is, those objects that change shape with pressure, are ubiquitous in our lives. Humans deal with deformable objects all the time when we fold laundry, pour ourselves a drink, and peel vegetables. Although such tasks are performed effortlessly by humans, they are challenging for traditional robotics techniques which assume that all objects are rigid. Robot deformable object manipulation, such as laundry folding or cooking, could enable those who cannot perform these tasks easily for themselves to live more independent lives. This Faculty Early Career Development (CAREER) project will develop novel methods for robots to learn to manipulate deformable objects. First, robots will learn to predict how their actions will cause an object to deform; robots will then use these predictions to determine how to achieve deformable object manipulation tasks. To enable the public to better understand this research, the investigators will develop a virtual robot interface called â€œTeach-a-Robot" that will enable anyone in the world to experience teaching a robot a new task: users will upload a video of themselves performing a task and then watch a robot attempting to imitate them. Finally, the investigators will design new course assignments that demonstrate the interplay between robotics, computer vision, and machine learning.&lt;br/&gt;&lt;br/&gt;The research objective of this project is to explore how self-supervised representation learning can be used by robots to achieve deformable object manipulation. Given a set of demonstrations of robots or people interacting with deformable objects, the project will enable robots to learn representations of these objects that are useful for manipulation. This effort will explore how self-supervised learning can be used to: predict the effect of a robot's actions on a deformable object; infer object properties; recognize deformable object parts under large deformations; learn to transfer policies between deformable objects of the same class; and learn visual representations that transfer well from simulation to the real world. The second research thrust will explore how robots can use these self-supervised representations to efficiently learn policies to manipulate deformable objects. This effort will develop novel methods for reinforcement learning that can perform multi-scale reasoning, enabling robots to focus on both task-relevant low-level object details as well as the global object configuration. This research effort will open new doors for robots to interact with a wide range of deformable objects, such as folding cloth, molding dough to make pastries, spreading sauce onto pizza dough, and peeling vegetables.&lt;br/&gt;&lt;br/&gt;This project is supported by the cross-directorate Foundational Research in Robotics program, jointly managed and funded by the Directorates for Engineering (ENG) and Computer and Information Science and Engineering (CISE).&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/04/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2046491</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Held</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>David Held</PI_FULL_NAME>
<EmailAddress>dheld@andrew.cmu.edu</EmailAddress>
<PI_PHON>6176453932</PI_PHON>
<NSF_ID>000762987</NSF_ID>
<StartDate>03/04/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>144Y</Code>
<Text>FRR-Foundationl Rsrch Robotics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>6840</Code>
<Text>ROBOTICS</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~397151</FUND_OBLG>
</Award>
</rootTag>
