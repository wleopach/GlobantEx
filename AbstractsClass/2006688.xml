<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:Small:Differential Testing for Machine Learning Software</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sol Greenspan</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Machine-learning systems, including deep-learning (DL) systems, demand reliability. DL systems consist of two key components: (1) models and algorithms that perform complex mathematical calculations, and (2) software that implements the algorithms and models. Here software includes DL infrastructure code (e.g., code that performs core neural-network computations) and the application code (e.g., code that loads model weights). Thus, for the entire DL system to be reliable, both the software implementation and models/algorithms must be reliable. If software fails to faithfully implement a model (e.g., due to a bug in the software), the output from the software can be wrong even if the model is correct, and vice versa. This project raises awareness of testing DL software implementations to find and localize defects in DL software. Specifically, this project develops end-to-end solutions to improve the reliability and robustness of DL systems by differential testing DL software (including both source code and data) to detect and localize bugs, and defend against adversarial input. In addition to advancing the state of the art, the findings, approaches, and tools developed in the project should provide educational and practical tools to test DL software. It could help transform how students, developers, and researchers test DL software. &lt;br/&gt;&lt;br/&gt;Testing DL software is challenging, as it is particularly difficult for developers to know the expected output of the software under test given an input instance, because DL algorithms and models use complex networks and mathematical formula. A second challenge is to identify the faulty functions among many in the DL software upon bug detection.  To address these challenges, this project uses differential testing to detect and localize bugs in DL software, including code and data, without relying on the expected output. To achieve this goal, one must understand the variances of multiple training runs with identical configurations, e.g., identical DL software, identical algorithm, identical training and test data, identical network, etc. Such variance indicates nondeterminism in the DL algorithms and software implementations, which imposes both opportunities and challenges for researchers and practitioners. Building on the variance results, the first thrust creates differential-testing approaches to detect software bugs in both the DL training and inference phases, as well as when multiple implementations are unavailable, e.g., by mutating models. It addresses the fundamental oracle challenge of testing DL software. The second thrust builds approaches to isolate the differences of multiple runs to localize bugs to help developers identify the bug root causes, so that developers can fix them correctly faster. The last thrust tests the data of DL software to identify and defend against adversarial input.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/29/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2006688</AwardID>
<Investigator>
<FirstName>Lin</FirstName>
<LastName>Tan</LastName>
<EmailAddress>lintan@purdue.edu</EmailAddress>
<StartDate>07/29/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Purdue University</Name>
<CityName>West Lafayette</CityName>
<ZipCode>479072114</ZipCode>
<PhoneNumber>7654941055</PhoneNumber>
<StreetAddress>Young Hall</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
</Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7944</Code>
<Text>SOFTWARE ENG &amp; FORMAL METHODS</Text>
</ProgramReference>
</Award>
</rootTag>
