<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CNS Core: Small: Mitigating Network Bottlenecks via Programmability for Distributed Machine Learning Systems</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>496595.00</AwardTotalIntnAmount>
<AwardAmount>496595</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Erik Brunvand</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Distributed machine learning (ML) is becoming an important way to allow multiple learning agents to train on separate slices of the same dataset simultaneously and exchange what they have learned with each other periodically over a network. Due to the significant bandwidth gap between network and processor units, the network is likely to become the bottleneck in these types of systems. To mitigate this issue, this project is developing a distributed ML algorithm and network system co-design to adapt training algorithms to make better use of network resources. First a programmable communication subsystem is proposed to accelerate training synchronization. Specifically, a comprehensive study on the impact of network congestion over distributed ML models will be conducted to provide unique insights. The project is also enhancing existing frameworks by integrating in-network control and exploring synchronization schemes that dynamically adjust learning hyper-parameters based on network signals. Next a scheduler that optimizes the utilization of heterogeneous computing resources is proposed. To that end, both deterministic and learning-based scheduling algorithms are being explored and a framework that enables operation-level scheduling for finer-grained control is being developed.  &lt;br/&gt;&lt;br/&gt;The proposed research investigates in-network control to mitigate network congestion which remains the biggest challenge for High Performance Computing (HPC) processors. It will significantly improve the training efficiency of the existing distributed training frameworks. In addition, the comprehensive and systematic studies will provide insights to the algorithm and system co-design solutions. The developed framework will also help students and researchers in their big data research projects. New courses will be developed based on the outcomes of the proposed work and new curriculum and training sessions on networking and distributed ML will be developed in High School Tech Camps during the summer. Source code, raw data, and simulation results generated in the project will be stored in standard formats and will be published in the public domain. All data will be archived on the departmental servers at Case Western Reserve University (CWRU) for increased availability and reliability.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/17/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2008468</AwardID>
<Investigator>
<FirstName>An</FirstName>
<LastName>Wang</LastName>
<EmailAddress>axw474@case.edu</EmailAddress>
<StartDate>08/17/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Case Western Reserve University</Name>
<CityName>CLEVELAND</CityName>
<ZipCode>441064901</ZipCode>
<PhoneNumber>2163684510</PhoneNumber>
<StreetAddress>Nord Hall, Suite 615</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Ohio</StateName>
<StateCode>OH</StateCode>
</Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
