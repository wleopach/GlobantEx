<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Fairness and Control of Exposure in Ranking</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>496762.00</AwardTotalIntnAmount>
<AwardAmount>496762</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sylvia Spengler</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Ranking functions trained via machine learning are ubiquitous in todayâ€™s online systems, where they are used to rank virtually anything - from products and movies to job candidates. By deciding where individual items get ranked and how easily users can find them, the ranking function greatly influences which products get purchased, which candidates get a job, and which movies get streamed. This raises questions of fairness, asserting that the ranking functions should be fair to both the users of the systems as well as to the items being ranked. The project develops new fairness criteria for ranking functions, as well as new methods for designing and learning ranking functions with fairness guarantees.&lt;br/&gt;&lt;br/&gt;The project is based on a model of ranking systems as two-sided markets, where utility goes not only to the users issuing the queries, but also to the items that are being ranked. Unfortunately, virtually all learning-to-rank (LTR) methods in use today only optimize the average utility to the users, which can lead to unfair treatment of the items and of minority user groups. To overcome this deficiency, the project develops LTR methods that can enforce desirable fairness constraints. These new methods can remedy disparate treatment of user groups (e.g. amplification of gender bias in a hiring system), market concentration in online markets, and the dynamics of participation in online systems (e.g. polarization). To achieve these goals, the project addresses both endogenous and exogenous causes of unfairness. Exogenous causes are due to biases in the training data, which often lead to rich-get-richer dynamics. However, even when trained with unbiased data, causes endogenous in the design of the LTR algorithm can lead to unfairness. Therefore, the LTR methods developed in the project address both endogenous and exogenous causes.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/04/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2008139</AwardID>
<Investigator>
<FirstName>Thorsten</FirstName>
<LastName>Joachims</LastName>
<EmailAddress>tj@cs.cornell.edu</EmailAddress>
<StartDate>09/04/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
