<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF:Small:Collaborative Research: Application-aware Energy Modeling and Power Management for Parallel and High Performance Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/15/2019</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>23273.00</AwardTotalIntnAmount>
<AwardAmount>23273</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Almadena Chtchelkanova</SignBlockName>
<PO_EMAI>achtchel@nsf.gov</PO_EMAI>
<PO_PHON>7032927498</PO_PHON>
</ProgramOfficer>
<AbstractNarration>One of the critical challenges in scaling out current and future high performance computing (HPC) and enterprise computing systems is the requirement that their power envelope remain comparable to that of today?s systems. This project addresses this ?power wall? challenge from the system software aspect by developing application-aware methodologies of energy modeling and power management. The project optimizes system efficiency by tuning performance and energy consumption to resonate with application runtime behavior while staying below the system power envelope. The project develops user interfaces and new compiler models and runtime tuning techniques to manage the tradeoffs between performance and energy consumption. The approach enables cooperative, application-specific control of energy consumption between hardware, system software and applications. The investigations and solutions deepen understanding of application power usage and guide users to customized performance and energy consumption behavior.&lt;br/&gt;&lt;br/&gt;This collaborative project integrates the development, education, and outreach efforts of collaborating University partners and is well positioned to have a substantial impact on both the HPC research community and hardware designers and vendors. All findings are published in peer-reviewed conferences and journals while source code and results are available through a project web site. This work addresses the need for energy efficiency improvements in large-scale systems in support of high-end simulations used to design pharmaceuticals, aircraft, global warming scenarios, etc. The proposed techniques influence the design of future directions HPC and enterprise computing systems from industry and government. The project engages and trains graduate and undergraduate students, including underrepresented minority students, in the area of energy efficient computing, parallel and high performance computing, and computer architecture and systems. The open source evaluation platforms are used in teaching related coursework in graduate and undergraduate classes.</AbstractNarration>
<MinAmdLetterDate>11/12/2019</MinAmdLetterDate>
<MaxAmdLetterDate>11/12/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2001580</AwardID>
<Investigator>
<FirstName>Yonghong</FirstName>
<LastName>Yan</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yonghong Yan</PI_FULL_NAME>
<EmailAddress>yyan7@uncc.edu</EmailAddress>
<PI_PHON>7046878546</PI_PHON>
<NSF_ID>000604781</NSF_ID>
<StartDate>11/12/2019</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Charlotte</Name>
<CityName>CHARLOTTE</CityName>
<CountyName>MECKLENBURG</CountyName>
<ZipCode>282230001</ZipCode>
<PhoneNumber>7046871888</PhoneNumber>
<StreetAddress>9201 University City Boulevard</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066300096</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHARLOTTE, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Charlotte]]></Name>
<CityName/>
<CountyName>MECKLENBURG</CountyName>
<StateCode>NC</StateCode>
<ZipCode>282230001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7798</Code>
<Text>Software &amp; Hardware Foundation</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7942</Code>
<Text>HIGH-PERFORMANCE COMPUTING</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~23273</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-61644fcf-7fff-b0dd-b7c2-998e4bd2f484"> </span></p> <p dir="ltr"><span>The goal of the project is to develop user interfaces and runtime tuning techniques to manage the tradeoffs between performance and energy consumption for parallel and high performance computing (HPC) applications. The approach is expected to enable cooperative, application-specific control of energy consumption between hardware, system software and applications. The investigations and solutions deepen understanding of application power usage and guide users to customized performance and energy consumption behavior.</span></p> <p dir="ltr"><span>With these goals, the project have the following outcomes:&nbsp;</span></p> <p dir="ltr"><span>1. Language extensions and compiler support have been created based on OpenMP to support distributing computation and data among multiple heterogeneous devices (CPU, NVIDIA GPU and Intel Xeon Phi MIC) on a computing node. These extensions enable more information sharing between programmers&rsquo; intension and system software for co-scheduling of data movement and computation to achieve maximum overlapping. It significantly improves programming productivity for users when using multiple devices for offloading computational tasks.&nbsp;</span></p> <p dir="ltr"><span>2. A runtime-based tuning framework has been created that is able to automatically tune the thread count and CPU frequency based on the performance and energy tradeoff and preference. The runtime API enables users to embed analysis, modeling and tuning capabilities in a parallel iterative program to guide computation simulation and solver at runtime. It has the potential to make a significant impact to the way computational simulation is performed in the future.</span></p> <p dir="ltr"><span>3. Black-box model and fine-grained model for combined analysis of performance and energy consumption of parallel and HPC applications have been created within the Aspen framework. The effectiveness of the models has been demonstrated for both energy prediction and optimization to enhance the energy efficiency of HPC applications.&nbsp;</span></p> <p dir="ltr"><span>4. A performance tool for HPC applications such as those created with MPI, OpenMP and CUDA has been created by integrating state-of-the-art Linux performance tools including LTTng, Eclipse, and runtime and profiling tool interface of existing programming models. This development demonstrates that leveraging commodity Linux tools for creating HPC-specific performance tools is viable and can greatly reduce the development cycle.&nbsp;</span></p> <p dir="ltr"><span>5. Existing scientific kernels, mini-apps, and proxy-apps that are widely used for HPC application experiments have been studied and enhanced with features and capability developed in this project to promote the adoption of the techniques produced by this project. All the software, tools, and experiment data produced during this project are released in the public domain or as open-source software. Some of the language extensions have been proposed to the OpenMP language committee to be considered as part of the standard.&nbsp;</span></p> <p dir="ltr"><span>6. Several Ph.D., master, and undergraduate students funded in this project are trained and graduated with skills of computer system, HPC applications, measurement of performance and power, and performance optimization.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 01/13/2021<br>      Modified by: Yonghong&nbsp;Yan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   The goal of the project is to develop user interfaces and runtime tuning techniques to manage the tradeoffs between performance and energy consumption for parallel and high performance computing (HPC) applications. The approach is expected to enable cooperative, application-specific control of energy consumption between hardware, system software and applications. The investigations and solutions deepen understanding of application power usage and guide users to customized performance and energy consumption behavior. With these goals, the project have the following outcomes:  1. Language extensions and compiler support have been created based on OpenMP to support distributing computation and data among multiple heterogeneous devices (CPU, NVIDIA GPU and Intel Xeon Phi MIC) on a computing node. These extensions enable more information sharing between programmersâ€™ intension and system software for co-scheduling of data movement and computation to achieve maximum overlapping. It significantly improves programming productivity for users when using multiple devices for offloading computational tasks.  2. A runtime-based tuning framework has been created that is able to automatically tune the thread count and CPU frequency based on the performance and energy tradeoff and preference. The runtime API enables users to embed analysis, modeling and tuning capabilities in a parallel iterative program to guide computation simulation and solver at runtime. It has the potential to make a significant impact to the way computational simulation is performed in the future. 3. Black-box model and fine-grained model for combined analysis of performance and energy consumption of parallel and HPC applications have been created within the Aspen framework. The effectiveness of the models has been demonstrated for both energy prediction and optimization to enhance the energy efficiency of HPC applications.  4. A performance tool for HPC applications such as those created with MPI, OpenMP and CUDA has been created by integrating state-of-the-art Linux performance tools including LTTng, Eclipse, and runtime and profiling tool interface of existing programming models. This development demonstrates that leveraging commodity Linux tools for creating HPC-specific performance tools is viable and can greatly reduce the development cycle.  5. Existing scientific kernels, mini-apps, and proxy-apps that are widely used for HPC application experiments have been studied and enhanced with features and capability developed in this project to promote the adoption of the techniques produced by this project. All the software, tools, and experiment data produced during this project are released in the public domain or as open-source software. Some of the language extensions have been proposed to the OpenMP language committee to be considered as part of the standard.  6. Several Ph.D., master, and undergraduate students funded in this project are trained and graduated with skills of computer system, HPC applications, measurement of performance and power, and performance optimization.          Last Modified: 01/13/2021       Submitted by: Yonghong Yan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
