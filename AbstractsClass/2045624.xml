<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Understanding the Relationship of Covert and Overt Attention Using Concurrent EEG and Eye Tracking</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2021</AwardEffectiveDate>
<AwardExpirationDate>07/31/2026</AwardExpirationDate>
<AwardTotalIntnAmount>708780.00</AwardTotalIntnAmount>
<AwardAmount>548133</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michael Hout</SignBlockName>
<PO_EMAI>mhout@nsf.gov</PO_EMAI>
<PO_PHON>7032922163</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In humans, visual search is crucial for everyday goal-directed behavior.  Whether scanning the road for hazards while driving or looking for a can of soup in a grocery store, people constantly search through complex visual environments to locate objects of interest.  Despite its importance, many basic questions about the brain processes involved in visual search remain unanswered.  One such question is how eye movements are controlled by the brain during visual search.  Humans generate rapid, ballistic eye movements called saccades which are used to focus attention on important objects.  These eye movements occur about 2-3 times per second and are generated approximately 100,000 times per day.  However, scientists are only beginning to understand how these saccadic eye movements are controlled by different brain areas.  The current project aims to bridge this gap in knowledge by developing a new technique to simultaneously measure eye movements (via infrared eye tracking) and neural activity (via electroencephalography; EEG).  Combining these two types of data will allow us to directly examine attentional processes in the brain that occur just before eye movements are generated.  This project will lead to a better understanding of the basic neuroscience underlying eye movements.  The knowledge gained from this project will also help provide key insights about visual search in applied settings such as radiological screening, transportation baggage security, and motor vehicle traffic control.&lt;br/&gt;&lt;br/&gt;This project will explore how the brain controls eye movements to search through complex images.  Prior research has identified two mechanisms of visual attention in humans.  First, saccadic eye movements are used to rapidly fixate objects of interest.  These ballistic eye movements occur many times per second and can be directly monitored using infrared eye-tracking cameras.  Second, humans can also covertly attend objects without moving the eyes.  That is, people can monitor an object “out of the corner of their eye” without directly looking at the object.  Interestingly, previous research has developed techniques to directly measure covert attention using EEG to noninvasively monitor neural activity from visual cortex in the brain.  However, a major shortcoming of this past research is that it used search tasks that prohibited eye movements because they result in noisy voltage fluctuations that make EEG waveforms impossible to interpret.  As a result, little is known about how the brain uses covert attention to coordinate eye movements during everyday visual search.  The current project aims to resolve this by developing a new technique to measure brain activity that occurs just before eye movement are generated.  The PI will measure shifts of covert attention (via EEG) and eye movements (via infrared eye tracking) concurrently.  During analysis, the PI will then examine the EEG waveform during the period of time prior to the eye movement for markers of covert attentional selection.  Specifically, the PI will assess whether the N2pc component—a gold-standard EEG measurement of attentional orienting—occurs just before eye movements are generated to a visual stimulus.  Additionally, this project will involve a virtual workshop to train promising young researchers approaches computer programming in cognitive neuroscience.  The ultimate goals of this project are to understand how covert attentional systems in the brain are used to guide eye movements during visual search and to develop better cognitive neuroscience models of visual attention in humans.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>04/13/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2045624</AwardID>
<Investigator>
<FirstName>Nicholas</FirstName>
<LastName>Gaspelin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Nicholas Gaspelin</PI_FULL_NAME>
<EmailAddress>gaspelin@binghamton.edu</EmailAddress>
<PI_PHON>6077774304</PI_PHON>
<NSF_ID>000830544</NSF_ID>
<StartDate>04/13/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Binghamton</Name>
<CityName>BINGHAMTON</CityName>
<ZipCode>139026000</ZipCode>
<PhoneNumber>6077776136</PhoneNumber>
<StreetAddress>4400 VESTAL PKWY E</StreetAddress>
<StreetAddress2><![CDATA[PO Box 6000]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>22</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY22</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>090189965</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Binghamton]]></Name>
<CityName>Binghamton</CityName>
<StateCode>NY</StateCode>
<ZipCode>139026000</ZipCode>
<StreetAddress><![CDATA[4400 Vestal Parkway East]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>22</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY22</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~548133</FUND_OBLG>
</Award>
</rootTag>
