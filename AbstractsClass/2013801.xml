<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>TWC SBE: Medium: Context-Aware Harassment Detection on Social Media</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>12/03/2019</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>13238.00</AwardTotalIntnAmount>
<AwardAmount>45238</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Sara Kiesler</SignBlockName>
<PO_EMAI>skiesler@nsf.gov</PO_EMAI>
<PO_PHON>7032928643</PO_PHON>
</ProgramOfficer>
<AbstractNarration>As social media permeates our daily life, there has been a sharp rise in the use of social media to humiliate, bully, and threaten others, which has come with harmful consequences such as emotional distress, depression, and suicide. The October 2014 Pew Research survey shows that 73% of adult Internet users have observed online harassment and 40% have experienced it. The prevalence and serious consequences of online harassment present both social and technological challenges. This project identifies harassing messages in social media, through a combination of text analysis and the use of other clues in the social media (e.g., indications of power relationships between sender and receiver of a potentially harassing message.) The project will develop prototypes to detect harassing messages in Twitter; the proposed techniques can be adapted to other platforms, such as Facebook, online forums, and blogs. An interdisciplinary team of computer scientists, social scientists, urban and public affairs professionals, educators, and the participation of college and high schools students in the research will ensure wide impact of scientific research on the support for safe social interactions.&lt;br/&gt;&lt;br/&gt;This project combines social science theory and human judgment of potential harassment examples from social media, in both school and workplace contexts, to operationalize the detection of harassing messages and offenders. It develops comprehensive and reliable context-aware techniques (using machine learning, text mining, natural language processing, and social network analysis) to glean information about the people involved and their interconnected network of relationships, and to determine and evaluate potential harassment and harassers. The key innovations of this work include: (1) identification of the generic language of insult, characterized by profanities and other general patterns of verbal abuse, and recognition of target-dependent offensive language involving sensitive topics that are personal to a specific individual or social circle; (2) prediction of harassment-specific emotion evoked in a recipient after reading messages by leveraging conversation history as well as sender's emotions; (3) recognition of a sender's malicious intent behind messages based on the aspects of power, truth (approximated by trust), and familiarity; (4) a harmfulness assessment of harassing messages by fusing aforementioned language, emotion, and intent factors; and (5) detection of harassers from their aggregated behaviors, such as harassment frequency, duration, and coverage measures, for effective prevention and intervention.</AbstractNarration>
<MinAmdLetterDate>05/28/2020</MinAmdLetterDate>
<MaxAmdLetterDate>05/28/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2013801</AwardID>
<Investigator>
<FirstName>Amit</FirstName>
<LastName>Sheth</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amit Sheth</PI_FULL_NAME>
<EmailAddress>amit@sc.edu</EmailAddress>
<PI_PHON>8037772094</PI_PHON>
<NSF_ID>000272511</NSF_ID>
<StartDate>05/28/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of South Carolina at Columbia</Name>
<CityName>COLUMBIA</CityName>
<CountyName>RICHLAND</CountyName>
<ZipCode>292080001</ZipCode>
<PhoneNumber>8037777093</PhoneNumber>
<StreetAddress>Sponsored Awards Management</StreetAddress>
<StreetAddress2>1600 Hampton Street, Suite 414</StreetAddress2>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<StateCode>SC</StateCode>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>SC06</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041387846</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTH CAROLINA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041387846</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of South Carolina at Columbia]]></Name>
<CityName/>
<CountyName>RICHLAND</CountyName>
<StateCode>SC</StateCode>
<ZipCode>292080001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>South Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>06</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>SC06</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1714</Code>
<Text>Special Projects - CNS</Text>
</ProgramElement>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>025Z</Code>
<Text>SaTC: Secure and Trustworthy Cyberspace</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9178</Code>
<Text>UNDERGRADUATE EDUCATION</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~13236</FUND_OBLG>
<FUND_OBLG>2016~16000</FUND_OBLG>
<FUND_OBLG>2017~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-27c7b007-7fff-d105-3f46-6db0e9bfe406"> <p dir="ltr"><span>Online harassment and toxic content have many negative impacts on human interaction and society as a whole. In extreme cases this leads&nbsp; individuals to depression and even suicide. Social media companies are trying to tackle this issue, and have reportedly hired thousands of human moderators to intercept and purge such content, so far with unsatisfactory results</span><span>. </span><span>Automated moderation is&nbsp; difficult due to the context sensitivity of human language, changes in the cultural acceptability of potentially harassing keywords, and relative sparsity of harassing content.</span></p> <p dir="ltr"><span>In this interdisciplinary work, we have studied online harassment by going beyond the key-word notations to identify the context of online harassment and toxicity. We employed several key concepts from social psychology such as conversation analysis, intentionality, and group phenomena to identify and analyze online harassment. We have developed and provided to the research community two unique datasets (a) a dataset covering different aspects of the problem (sexual, racial, appearance, intelligence, politics, generic), and (b) a tagged dataset of anonymized online content of adolescent conversations, capturing both context and their language preferences.&nbsp; Our own analyses of these datasets enriches the understanding of the harassment problem in at least two respects. First context matters in the assessment of harassing content, including prior relationships between the participants, community membership (and the insider-outsider phenomenon in particular), cohort linguistic practices and the presence of exonerating content (to distinguish threatening tweets from superficially harassing curse-word laden friendly banter).&nbsp; Second, harassment sparsity not only threatens the practicality of machine learning classifiers, but also the risk of false alarms given a high base rate of non-harassing content (i.e., sparsity of positive cases). &nbsp; Both properties of the harassment problem require a top-down knowledge-based approach to the detection of harassment in contrast to the more prevalent data driven efforts.</span></p> <p dir="ltr"><span>To overcome data sparsity in harassment we have examined detection issues in a related model domain of antisocial behavior: extremist communication.&nbsp; We show how&nbsp; knowledge bases for religion, ideology and hate support computational detection of extremists. This knowledge based approach successfully excludes likely mislabeled users and therefore addresses the false alarm problem. This knowledge based approach also enhances computational community detection, relevant to the identification of toxic user groups in online communities. The project provided extensive opportunity for training for students and postdocs from computer science as well as cognitive science in a highly inclusive, interdisciplinary setting.</span></p> <div><span><br /></span></div> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 11/15/2020<br>      Modified by: Amit&nbsp;Sheth</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Online harassment and toxic content have many negative impacts on human interaction and society as a whole. In extreme cases this leads  individuals to depression and even suicide. Social media companies are trying to tackle this issue, and have reportedly hired thousands of human moderators to intercept and purge such content, so far with unsatisfactory results. Automated moderation is  difficult due to the context sensitivity of human language, changes in the cultural acceptability of potentially harassing keywords, and relative sparsity of harassing content. In this interdisciplinary work, we have studied online harassment by going beyond the key-word notations to identify the context of online harassment and toxicity. We employed several key concepts from social psychology such as conversation analysis, intentionality, and group phenomena to identify and analyze online harassment. We have developed and provided to the research community two unique datasets (a) a dataset covering different aspects of the problem (sexual, racial, appearance, intelligence, politics, generic), and (b) a tagged dataset of anonymized online content of adolescent conversations, capturing both context and their language preferences.  Our own analyses of these datasets enriches the understanding of the harassment problem in at least two respects. First context matters in the assessment of harassing content, including prior relationships between the participants, community membership (and the insider-outsider phenomenon in particular), cohort linguistic practices and the presence of exonerating content (to distinguish threatening tweets from superficially harassing curse-word laden friendly banter).  Second, harassment sparsity not only threatens the practicality of machine learning classifiers, but also the risk of false alarms given a high base rate of non-harassing content (i.e., sparsity of positive cases).   Both properties of the harassment problem require a top-down knowledge-based approach to the detection of harassment in contrast to the more prevalent data driven efforts. To overcome data sparsity in harassment we have examined detection issues in a related model domain of antisocial behavior: extremist communication.  We show how  knowledge bases for religion, ideology and hate support computational detection of extremists. This knowledge based approach successfully excludes likely mislabeled users and therefore addresses the false alarm problem. This knowledge based approach also enhances computational community detection, relevant to the identification of toxic user groups in online communities. The project provided extensive opportunity for training for students and postdocs from computer science as well as cognitive science in a highly inclusive, interdisciplinary setting.             Last Modified: 11/15/2020       Submitted by: Amit Sheth]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
