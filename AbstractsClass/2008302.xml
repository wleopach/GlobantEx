<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI:Small: Capturing, Perceiving, and Rendering of Artistic Skills for Real-time Interactive Creation of Art</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>449486.00</AwardTotalIntnAmount>
<AwardAmount>140816</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>David Miller</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Today’s rapid technological advances have raised fears that humanity will lose relevance, that automated machines will eventually outpace human physical capabilities, human thought, and even human creativity, that humanity will be on the losing side of the emerging technology gap. This project will provide new methods for human-robot collaboration in artistic expression, as a step toward bridging this technology gap in domains where creativity is paramount. For instance, consider a master painter with decades of practice on canvas; preserving their mastery of skills and creativity is extremely challenging, let alone replicating their masterpieces. The team will address a myriad of technical challenges associated with capturing artist skills, recognizing their creative intents, and robotically generating art in the artist’s own styles and techniques. Calligraphy presents a complex set of technical challenges, as it involves contact of a soft, deformable brush with a paper surface to apply liquid ink. The team will investigate algorithmic solutions for skillful use of such complex artist mediums by robots. The developed robotic systems will be publicly demonstrated in collaborative performances with professional artists; these dissemination activities will demonstrate how a proficient technology could preserve and amplify the creative status of the human artist.&lt;br/&gt;&lt;br/&gt;The long-term goal of the project is to achieve genuine human-robot collaboration during the creative, artistic process.  Progress toward this goal will proceed along four separate thrusts, each focused on pushing the state of the art in a particular direction.  The first step is to build systems that, in some sense, understand the artist’s creative process. For the purposes of this project, understanding can be demonstrated by capturing, perceiving, and rendering the dexterous skills of human artists. The artist’s motions will be recorded using motion capture technology (similar to methods used in modern cinema), and the captured data will then be used to design mathematical models that will form the basis for reasoning about the artist’s actions, and for building perception algorithms that can understand the artist’s motion in real time. Hence, the first research thrust, capture, will develop representations of artistic skills, and algorithms to enable the recording of the creation of an artifact via skillful manipulation of a tool.  The second thrust, perception, will develop algorithms that can, in real-time, sense the artistic intent of an artist, using learned representations, requiring much less comprehensive sensing modalities. The third thrust, rendering, will develop identification and control algorithms that allow a robot to render artistic actions in different contexts, e.g., after creation of an artwork is captured or in the context of an interactive system.  The final step is designing robotic systems that are capable of rendering these skills, with the goal of enabling genuine human-robot collaboration during the creative process.  This symbiotic relationship preserves the creative status of the human artist, while providing augmented capabilities to enhance the creation process. Finally, the fourth thrust is to integrate all of these results into a truly interactive system, where both perception and control are leveraged to enable human-robot collaboration in creating novel works of art.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/02/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2008302</AwardID>
<Investigator>
<FirstName>Seth</FirstName>
<LastName>Hutchinson</LastName>
<EmailAddress>seth@gatech.edu</EmailAddress>
<StartDate>09/02/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Frank</FirstName>
<LastName>Dellaert</LastName>
<EmailAddress>dellaert@cc.gatech.edu</EmailAddress>
<StartDate>09/02/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sang-won</FirstName>
<LastName>Leigh</LastName>
<EmailAddress>sang.leigh@design.gatech.edu</EmailAddress>
<StartDate>09/02/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
