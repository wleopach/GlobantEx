<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Frontiers of Distributed Machine Learning with Communication, Computation and Data Constraints</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/01/2021</AwardEffectiveDate>
<AwardExpirationDate>01/31/2026</AwardExpirationDate>
<AwardTotalIntnAmount>650000.00</AwardTotalIntnAmount>
<AwardAmount>270054</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Scott Acton</SignBlockName>
<PO_EMAI>sacton@nsf.gov</PO_EMAI>
<PO_PHON>7032922124</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The meteoric success of machine learning (ML) during the past decade can be attributed to the confluence of two key factors: big compute and big data. For example, although neural network models were proposed several decades ago, they came into the mainstream only after the advent of affordable cloud computing, and the availability of massive training datasets. The state-of-the-art approach towards distributed training is to centrally shuffle data and then partition them across nodes equipped with powerful computation units and high-speed communication links. The indispensability of such communication-, compute- and data-intensive frameworks precludes resource-limited organizations from using current ML algorithms. This project seeks to democratize ML by enabling it to seamlessly scale to a network of computation-, communication-, and data-constrained nodes. Expected outcomes include distributed training and inference algorithms that are system-aware (robust to communication and computation limitations) and data-aware (can handle statistically skewed and scarce data). The research outcomes will be complemented by undergraduate, graduate, and high-school outreach classes and a monograph on large-scale learning. The investigator also aims to host an annual collaboration workshop for female researchers to create collaboration and mentorship opportunities for women in STEM.&lt;br/&gt;&lt;br/&gt;This project consists of three research thrusts related to communication, computation, and data constraints, respectively. The first thrust develops several facets of communication efficiency in distributed training to achieve an order-of-magnitude reduction in training time. The second thrust will tackle computational heterogeneity, which can cause consistency and scalability issues in model training and inference. The investigator will design distributed training, inference, and hyper-parameter optimization algorithms robust to such heterogeneity. The third thrust will address fundamental problems that stem from data heterogeneity, such as adaptive node selection, fairness, personalization, and data-scarce learning. Rather than improving the system and the algorithms in isolation, the investigator will take a system-aware and data-aware approach that draws novel insights from scheduling, coding theory, and multi-armed bandits. She will also collaborate with Google's federated learning team to validate the research outcomes and amplify their impact.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>01/08/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2045694</AwardID>
<Investigator>
<FirstName>Gauri</FirstName>
<LastName>Joshi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Gauri Joshi</PI_FULL_NAME>
<EmailAddress>gaurij@andrew.cmu.edu</EmailAddress>
<PI_PHON>4122681186</PI_PHON>
<NSF_ID>000732900</NSF_ID>
<StartDate>01/08/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133890</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7797</Code>
<Text>COMM &amp; INFORMATION FOUNDATIONS</Text>
</ProgramReference>
<ProgramReference>
<Code>7937</Code>
<Text>NETWORK CODING AND INFO THEORY</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~270054</FUND_OBLG>
</Award>
</rootTag>
