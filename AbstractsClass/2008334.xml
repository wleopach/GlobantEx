<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>III: Small: Go Beyond Short-term Dependency and Homogeneity: A General-Purpose Transformer Recipe for Multi-Domain Heterogeneous Sequential Data Analysis</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>499999.00</AwardTotalIntnAmount>
<AwardAmount>327169</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei-Shinn Ku</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Sequential data are ubiquitous in domains including healthcare, cyber security, social science, and online search and recommendation systems. Deep learning techniques have recently demonstrated tremendous successes in sequential data analysis tasks, such as sequential prediction, and text understanding, and times series classification and clustering. However, modern sequential data created from many domains are becoming ever more massive, complex, and domain-specific. When handling such complex and multi-domain sequential data, existing deep learning methods are limited in capturing long-term dependency and generalizing to multiple domains. To bridge such a gap, this project will develop principled algorithms and methodologies that can handle data heterogeneity and long-term dependencies for analyzing complex and multi-domain sequential data. The proposed framework will be generic to various types of sequential data including human language, time series, and trajectory data. It will open new possibilities of enabling deep learning techniques for more challenging sequential data analysis applications in social network analysis, clinical care, smart transportation, text mining, and natural language processing.&lt;br/&gt;&lt;br/&gt;The proposed framework leverages the popular transformer architecture and incorporates multi-domain adaptation to handle the data heterogeneity.  Specifically, this project aims to develop (i) unsupervised learning techniques for transformer-based multi-domain point process analysis, (ii) transformer-based metric learning techniques that enable large-scale and multi-domain time series analysis, and (iii) techniques for robust and efficient domain transfer learning over pre-trained transformers. The developed techniques will enjoy both computational efficiency and modeling flexibility of capturing long-term dependency and data heterogeneity, by addressing the computational and statistical challenges for these problems. The proposed research will also deliver open-source software in the form of easy-to-use libraries, which facilitate researchers and practitioners in related fields to analyze complex sequential data.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/13/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2008334</AwardID>
<Investigator>
<FirstName>Tuo</FirstName>
<LastName>Zhao</LastName>
<EmailAddress>tzhao80@gatech.edu</EmailAddress>
<StartDate>08/13/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Chao</FirstName>
<LastName>Zhang</LastName>
<EmailAddress>chaozhang@gatech.edu</EmailAddress>
<StartDate>08/13/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Georgia Tech Research Corporation</Name>
<CityName>Atlanta</CityName>
<ZipCode>303320420</ZipCode>
<PhoneNumber>4048944819</PhoneNumber>
<StreetAddress>Office of Sponsored Programs</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
</Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
