<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Development of a midwave infrared plenoptic camera prototype</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/15/2020</AwardEffectiveDate>
<AwardExpirationDate>04/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>225000.00</AwardTotalIntnAmount>
<AwardAmount>225000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Steven Konsek</SignBlockName>
<PO_EMAI>skonsek@nsf.gov</PO_EMAI>
<PO_PHON>7032927021</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to demonstrate the viability of a new camera solution for night or day and in adverse weather. Today's automatic driver assist systems are often inadequate for identifying pedestrians at night or in poor conditions.  This project will demonstrate a camera solution that will operate day and night, in all weather, and detects and classifies pedestrians, cyclists, animals as well as vehicles (either moving or stationary), thereby unlocking safer vehicle operation and saving lives.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project addresses a shortcoming of contemporary autonomous vehicle sensors by introducing a high-resolution 3D sensor that performs well in any lighting condition and most weather conditions. The research builds upon light field, or plenoptic, technologies proven effective in the visible domain and applies them to the thermal domain with additional innovations. A purpose-built microlens array (MLA) is coupled with a mid-wave infrared (MWIR) camera and new thermal computational photogrammetry algorithms to deliver dense 2D and range information with sufficient detail to quickly classify surrounding objects. In particular, it will be shown that 3D MWIR video is ideally suited to readily detect and classify objects that may move such as pedestrians, vehicles and animals.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>05/14/2020</MinAmdLetterDate>
<MaxAmdLetterDate>05/14/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2014933</AwardID>
<Investigator>
<FirstName>Eugene</FirstName>
<LastName>Petilli</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eugene M Petilli</PI_FULL_NAME>
<EmailAddress>genep@owlai.us</EmailAddress>
<PI_PHON>5855036084</PI_PHON>
<NSF_ID>000808708</NSF_ID>
<StartDate>05/14/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>OWL AUTONOMOUS IMAGING INC</Name>
<CityName>FAIRPORT</CityName>
<CountyName/>
<ZipCode>144504202</ZipCode>
<PhoneNumber>5857218168</PhoneNumber>
<StreetAddress>562 WILLOWBROOK OFFICE PARK</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY25</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>111562196</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>OWL AUTONOMOUS IMAGING INC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[OWL AUTONOMOUS IMAGING INC]]></Name>
<CityName>Fairport</CityName>
<CountyName/>
<StateCode>NY</StateCode>
<ZipCode>144504202</ZipCode>
<StreetAddress><![CDATA[562 Willowbrook Office Park]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>25</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY25</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>8034</Code>
<Text>Hardware Components</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~225000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Owl Autonomous Imaging, Inc. has successfully developed and demonstrated a monocular 3D thermal camera. This new sensing modality confers both sensor diversity and redundancy to the de factor sensor suite commonly used in autonomous, semi-autonomous and robotic mobility applications and holds the promise of improving both the safety and usability of emerging mobility technologies.</p> <p>&nbsp;</p> <p>In this Phase 1| SBIR, several means for generating range data from a monocular thermal imager were developed, evaluated and tested. These approaches included both optics-based (e.g., physics-based) means as well as Convolutional Neural Network (CNN) - based ranging techniques.&nbsp; Multi-aperture optics were investigated and evaluated for suitability to support both optics and CNN ranging techniques. Several monocular CNN ranging techniques were investigated, evaluated and prototyped for thermal imagery thereby pioneering the new field of thermal monocular CNN ranging. Ground truth and empirical data were collected from indoor controlled environments, outdoor staged engagements and an on-vehicle on-road testbed. Results confirm the feasibility of monocular 3D ranging and help point the way to continued improvement of this early-stage and highly-promising technology.</p> <p>&nbsp;</p> <p>Thermal imagers operating in the midwave (3-5 micron) or longwave (8-12 micron) bands are uniquely suited to sustain imaging in the presence of environmental challenges like glare, smoke, dust, fog and haze that may momentarily compromise or neutralize reflected-light sensors such as cameras (visible ? SWIR) and LiDARs. Likewise, 3D data is vital to mobility applications to help classify objects as well as ascertain their position, velocity and path. The 3D monocular thermal camera developed in this grant demonstrates not only effective 2D thermal imagery and range data but delivers these data sets in a pre-fused fashion where 2D and 3D datasets are perfectly geospatially registered with zero computational load by nature of the monocular sensor. Furthermore, being a purely passive sensor eliminates eye safety and sensor crosstalk concerns.&nbsp; Passive sensors are also inherently stealthy making them attractive for defense applications.</p> <p>&nbsp;</p> <p>This technology has broad applicability to several industries that require robust 3D imaging in ground-level environmental conditions including no light, glare, dust, smoke and haze. Applications include off-road heavy equipment operation such as mining and agriculture, defense applications such as off-road navigation and on-road applications such as ADAS, semi-autonomous and autonomous passenger and commercial vehicle operation.</p> <p>&nbsp;</p><br> <p>            Last Modified: 05/05/2021<br>      Modified by: Eugene&nbsp;M&nbsp;Petilli</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Owl Autonomous Imaging, Inc. has successfully developed and demonstrated a monocular 3D thermal camera. This new sensing modality confers both sensor diversity and redundancy to the de factor sensor suite commonly used in autonomous, semi-autonomous and robotic mobility applications and holds the promise of improving both the safety and usability of emerging mobility technologies.     In this Phase 1| SBIR, several means for generating range data from a monocular thermal imager were developed, evaluated and tested. These approaches included both optics-based (e.g., physics-based) means as well as Convolutional Neural Network (CNN) - based ranging techniques.  Multi-aperture optics were investigated and evaluated for suitability to support both optics and CNN ranging techniques. Several monocular CNN ranging techniques were investigated, evaluated and prototyped for thermal imagery thereby pioneering the new field of thermal monocular CNN ranging. Ground truth and empirical data were collected from indoor controlled environments, outdoor staged engagements and an on-vehicle on-road testbed. Results confirm the feasibility of monocular 3D ranging and help point the way to continued improvement of this early-stage and highly-promising technology.     Thermal imagers operating in the midwave (3-5 micron) or longwave (8-12 micron) bands are uniquely suited to sustain imaging in the presence of environmental challenges like glare, smoke, dust, fog and haze that may momentarily compromise or neutralize reflected-light sensors such as cameras (visible ? SWIR) and LiDARs. Likewise, 3D data is vital to mobility applications to help classify objects as well as ascertain their position, velocity and path. The 3D monocular thermal camera developed in this grant demonstrates not only effective 2D thermal imagery and range data but delivers these data sets in a pre-fused fashion where 2D and 3D datasets are perfectly geospatially registered with zero computational load by nature of the monocular sensor. Furthermore, being a purely passive sensor eliminates eye safety and sensor crosstalk concerns.  Passive sensors are also inherently stealthy making them attractive for defense applications.     This technology has broad applicability to several industries that require robust 3D imaging in ground-level environmental conditions including no light, glare, dust, smoke and haze. Applications include off-road heavy equipment operation such as mining and agriculture, defense applications such as off-road navigation and on-road applications such as ADAS, semi-autonomous and autonomous passenger and commercial vehicle operation.          Last Modified: 05/05/2021       Submitted by: Eugene M Petilli]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
