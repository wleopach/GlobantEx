<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Learning to Anticipate with Visual Simulation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2021</AwardEffectiveDate>
<AwardExpirationDate>08/31/2026</AwardExpirationDate>
<AwardTotalIntnAmount>550000.00</AwardTotalIntnAmount>
<AwardAmount>115280</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this research project is to develop methods which are able to accurately forecast what an observed scene looks like a few seconds from now. For instance, given frames of a video showing a traffic intersection, how can a machine anticipate the situation at the intersection a few seconds after the last observed video frame? Humans have a remarkable ability to address this task, which is used permanently, e.g., to safely navigate at an intersection, to effectively collaborate in a kitchen, and even when reading. To address this task, neuroscience hypothesizes that the situation is simulated using mental models. This helps to quickly converge to a set of likely outcomes while ruling out implausible situations. In contrast, present-day computer vision, machine learning and autonomous systems which address this challenge are at their infancy. While the last decade has shown tremendous progress for tasks which analyze observations, e.g., to detect visible objects and segment their contours, present-day systems are challenged when reasoning about something that is not directly observed, e.g., the situation a few seconds from now. Reasoning about the unobserved is challenging because the number of possibilities grows quickly. Yet, the ability to forecast is important for any system that wants to interact safely with its surroundings. To close this gap and lay the foundations for systems to anticipate, this project studies three aspects: 1) representations of the data which are suitable for forecasting, 2) properties of methods that permit accurate forecasting, and 3) what data is necessary to develop accurate models for forecasting.&lt;br/&gt;&lt;br/&gt;Technically, to address the aforementioned three aspects, the project develops methods which learn how to anticipate via visual simulation. Specifically, the methods use the observed data to retrieve a model of the scene either explicitly or implicitly (Thrust 1). The methods also learn from data how this model is transformed to match likely futures, i.e., the systems learn to perform visual simulation. For this, the methods disentangle geometry, dynamics and relations between observed entities via latent variables (Thrust 2). Disentangling is important because geometry, dynamics and relations influence futures differently. The amount and detail of the annotated data which is used to develop these methods will affect the outcomes. This project studies those relations by collecting a novel dataset (Thrust 3). The representations, algorithms and data innovations will be incorporated into undergraduate and graduate courses as well as an outreach program which is developed to teach audience-centric presentations to undergraduate and graduate students, providing an opportunity to learn to anticipate audience behavior (Thrust 4).&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>03/10/2021</MinAmdLetterDate>
<MaxAmdLetterDate>05/20/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2045586</AwardID>
<Investigator>
<FirstName>Alexander</FirstName>
<LastName>Schwing</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Alexander Schwing</PI_FULL_NAME>
<EmailAddress>aschwing@illinois.edu</EmailAddress>
<PI_PHON>2173332187</PI_PHON>
<NSF_ID>000734749</NSF_ID>
<StartDate>03/10/2021</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Illinois at Urbana-Champaign</Name>
<CityName>Champaign</CityName>
<ZipCode>618207406</ZipCode>
<PhoneNumber>2173332187</PhoneNumber>
<StreetAddress>1901 South First Street</StreetAddress>
<StreetAddress2><![CDATA[Suite A]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>041544081</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ILLINOIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041544081</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Illinois at Urbana-Champaign]]></Name>
<CityName>Urbana</CityName>
<StateCode>IL</StateCode>
<ZipCode>618013620</ZipCode>
<StreetAddress><![CDATA[506 S. Wright St.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2021~115280</FUND_OBLG>
</Award>
</rootTag>
